> Status tracker moved to `meta/design/v2/v4-phase1-progress.md` (updated 2026-02-14).

i have some doubts about the coding of the laws, like really a dsl was needed after all, seems like quite of work to make it palattable, i want something like pyton but also like curly braced for scopes

Yeah, that doubt is healthy. DSLs are expensive because you’re secretly building a language *and* a community of future-you who must live with it.

The good news: you don’t have to choose between “pure JSON rulesets” and “full custom DSL.” There’s a middle path that keeps your invariants (canonicalization, witness identity, lens versioning) while giving you something that *feels* like Python with curly-braced scope blocks.

### The key idea

Separate **authoring syntax** from **semantic core**:

* Keep your current canonical IR + CBOR hashing as the “law.”
* Allow multiple *frontends* that compile to the same IR.
* Start with a “Pythonic” surface that is intentionally small.

So the question becomes: *what is the smallest authoring language that makes humans happy without turning into a compiler project?*

---

## Option A: “ADM-lite” becomes Pythonic (recommended)

Keep `.adm`, but evolve it to look like:

* Python expressions for predicates / conditions
* Curly braces only for blocks (scope/lens/constraint)
* No fancy macros, no complex type system

Example:

```adm
lens software@0 {
  meta.title = "Software Starter Lens"

  constraint tests_required {
    when merge_boundary
    require ci.tests_passed
      else inadmissible("tests_failed")
  }

  constraint workflow_edit_requires_manual {
    when git.touched(".github/workflows/**")
    require manual_approval("workflow_edit")
      else route bucket:trust_debt cost 2 "workflow_edit"
  }
}
```

This is very “Pythonic” in feel:

* dotted access
* function-ish predicates
* readable `require … else …`

And curly braces give you the *scope boundary vibe* you want.

**Why this is good:** you only need to parse:

* identifiers
* string/number literals
* function calls
* boolean ops
* block structure

It’s a small grammar.

---

## Option B: Don’t build a language: embed in Python

You can author lenses in actual Python, then compile to IR.

Example:

```py
from admit import lens, constraint, route, require

with lens("software", 0):
    with constraint("tests_required"):
        require(ci.tests_passed(), reason="tests_failed")

    with constraint("workflow_edit"):
        when(git.touched(".github/workflows/**"))
        require(manual_approval("workflow_edit"))
        route("bucket:trust_debt", 2, "workflow_edit")
```

Pros:

* no parser
* users already know Python
* you get tooling for free (formatters, linting, editor support)

Cons:

* determinism risks unless you restrict the runtime hard (no arbitrary Python)
* you still need an extraction layer to canonical IR
* now you’re shipping a Python runtime dependency

This can work if you treat it as a **declarative builder API** and forbid general computation.

---

## Option C: Use an existing config language (HCL / Cue / Dhall)

If you want “structured + braces” without writing a parser:

* HCL (Terraform style) is very readable and blocky
* Cue is schema-first and powerful
* Dhall is typed and deterministic (but heavier)

Example HCL-ish:

```hcl
lens "software" "0" {
  constraint "workflow_edit" {
    when = git.touched(".github/workflows/**")
    require_manual_approval = true
    route = { bucket = "bucket:trust_debt", cost = 2, unit = "workflow_edit" }
  }
}
```

Pros:

* no custom syntax design
* existing parsers

Cons:

* expression language may not match your needs
* you’ll still map into IR

---

## What I’d recommend for *your* project

Do Option A: **ADM-lite**.

Because:

* you already have `.adm` and Chumsky
* you already have IR + canonicalization discipline
* you want something that reads like “laws” and feels ergonomic

The move is not “ditch DSL,” it’s:
**constrain the DSL until it’s basically a tiny declarative language.**

---

## A concrete minimal grammar that feels Pythonic

Here’s the smallest set of constructs that buys you a lot:

1. Blocks with braces:

* `lens <id>@<ver> { ... }`
* `scope <id>@<ver> { ... }`
* `constraint <name> { ... }`

2. Expressions:

* `and`, `or`, `not`
* function calls: `git.touched("glob")`, `ci.tests_passed()`
* dotted names: `bucket.trust_debt`

3. Actions inside constraints:

* `require <expr> else <action>`
* actions:

  * `inadmissible("reason")`
  * `route bucket:<id> cost <n> "<unit>" because "<text>"`
  * `manual_approval("<reason>")`

That’s enough to encode your starter lens + CIv3 rules cleanly.

---

## Why a DSL *was* needed (but a small one)

If your core invariant is “witnessed, versioned, audited laws,” you need:

* spans/attribution
* deterministic lowering
* stable hashes
* controlled semantics

General-purpose Python fights you on all of those unless you sandbox it hard.

So: a DSL is justified *if it stays small* and compiles to a canonical IR.

---

## The pragmatic path forward

1. Keep the ruleset JSON as the stable compiled target for now.
2. Implement “ADM-lite” as an authoring frontend that compiles to that JSON/IR.
3. Don’t add features unless they reduce authoring friction measurably.

In other words: treat the DSL as a UI layer, not a new religion.

If you want, I can take your current `.admit/rulesets/software-lens-v0.json` and rewrite it into an “ADM-lite” version in this Pythonic+braces style, so you can see if it actually feels better before committing to parser work.

the dsl sounds the most reasonable but my doubts are with making it extensible using the idea of scope providers, that can declare predicates and so as some sort of libraries for composition 

Yep — *that’s* the real DSL anxiety: not syntax, but **extensibility without turning into a plugin swamp**.

The good news is you can make this clean if you treat scope providers like “dynamic libraries with typed contracts,” and keep the DSL’s job strictly to: **bind, compose, and govern**—not compute.

Here’s a robust model that stays sane.

---

## The core separation (non-negotiable)

### Providers do computation

* “What files changed?”
* “Did required checks pass?”
* “Is public API changed?”
* “Does Cargo.toml change without Cargo.lock?”

### DSL does composition + governance

* “If workflow files touched → require manual approval”
* “If deps changed without lockfile → inadmissible”
* “If unknown → route explanation debt”

So: **Providers produce facts/predicates. DSL arranges them into laws.**

---

## What “extensible” should mean

Extensibility should mean:

1. People can add new *scopes/predicates* as libraries
2. Repos can import them and compose them
3. The runtime can validate + execute them deterministically
4. Lens updates remain governed (no silent “library drift”)

That’s basically “package management,” but for predicate contracts.

---

## The missing primitive: a “Scope Pack”

Introduce a first-class artifact:

**`scope-pack/0`** (or just entries in `meta-registry/1`) that contains:

* `scope_id@version`
* predicate list + signatures
* param schemas (types)
* result type (`bool3`, `u32`, `string`, `set[path]`, etc.)
* backend descriptor (rust crate / wasm / http / command)
* determinism flag and stability notes
* hash of the pack contents (canonical)

Then orgs publish packs, and repos import them.

### DSL import

```adm
import scope_pack "org/github@2"
import scope_pack "org/rust@1"
```

Now composition is just naming.

---

## How providers “declare predicates” safely

Do **not** let providers arbitrarily define semantics at runtime without a contract.

Instead: each provider ships a **ProviderDescriptor** that is itself content-addressed (or registry-registered):

* `provider_id`
* `scope_id`
* `predicates[]` with:

  * `predicate_id`
  * `params_schema`
  * `result_type`
  * `evidence_shape` (what findings look like)
  * `deterministic: bool`
  * `version`

Your `meta-registry/1` should either:

* include this descriptor, or
* reference it by hash (and store it as an artifact)

So the system always knows exactly “what predicates exist” under a scope version.

---

## Composition libraries in DSL: “derived predicates”

This is the sweet spot for reuse *without* new code.

Allow DSL to define **derived predicates** (pure composition over existing predicates):

```adm
derive predicate risky_change() -> bool3 =
  git.touched(".github/workflows/**")
  or git.touched(".admit/**")
  or git.sensitive_path_touched(patterns: std.sensitive_patterns)
```

No new computation. Just composition.

That gives you 80% of “library” power with 20% complexity.

Org “policy packs” can then be mostly:

* derived predicates
* lens fragments (“include these constraints”)

---

## Versioning: the real hard part

Two things can change over time:

1. The lens (rules)
2. The provider pack (predicate semantics / extraction behavior)

If you don’t govern #2, you get silent drift through a side door.

### So treat provider updates as meta-changes

Changing any of:

* predicate signature
* determinism flag
* evidence shape
* backend behavior that affects outputs

…should produce a `MetaChange` event (or at least a registry update witness) and support `InterpretationDelta` on a pinned snapshot.

**Rule of thumb:**

* lens changes are “policy amendments”
* provider changes are “measurement instrument changes”

The latter is often more dangerous.

---

## Determinism: you need a type for “this can be unknown”

Some providers (GitHub API, networked CI status) are not perfectly stable.

So bake this into predicate result types:

* `bool3 = true | false | unknown`
* and make DSL handle `unknown` explicitly:

  * route `explanation_debt`
  * or require manual approval
  * or treat as inadmissible in enforce mode

That’s how you stay honest while still being usable in CI.

---

## Runtime resolution: how the engine finds implementations

In practice you want **two registries**:

### 1) Semantic registry (meta-registry)

* defines scopes/predicates and versions
* used for validation, typing, governance
* content-addressed

### 2) Provider registry (runtime)

* maps `(scope_id@version)` → implementation backend
* local to environment (CI runner)
* can be:

  * “built-in providers” (Rust crates compiled in)
  * “wasm providers in ./providers”
  * “http providers configured by URL”

Your engine validates against #1 and executes via #2.

This is exactly how compilers separate “typechecking” from “linking.”

---

## How this feels in DSL

A repo author should write:

```adm
import scope_pack "std/git@0"
import scope_pack "std/deps@0"
import scope_pack "org/github@2"

lens software@1 {
  include "org/policy/software-guardrails@3"

  constraint workflow_edits {
    when git.touched(".github/workflows/**")
    require manual_approval("workflow_edit")
  }

  constraint deps_lockfile {
    when deps.manifest_changed_without_lockfile(changed_paths: git.changed_paths())
    inadmissible("lockfile_missing")
  }
}
```

Notice:

* scope packs provide the primitives
* lens composes them
* governance stays in lens/meta-change
* no magic.

---

## A practical adoption rule: keep “provider packs” boring

Your extensibility will succeed if provider packs:

* are small
* have stable names
* have tight contracts
* publish version bumps conservatively
* ship golden fixtures for determinism-sensitive predicates

And your system will remain sane if:

* any provider pack update emits a witness and can be diffed (delta).

---

## What to implement first (minimal but correct)

1. Add **predicate signatures** + param schemas to your existing provider descriptors
2. Allow DSL to **import** those descriptors (from registry)
3. Add **derived predicates** (composition only)
4. Add a “pack version bump” governance flow (registry update witness + optional delta)

That gives you extensibility without “plugin chaos.”

---

If you show me what your current `ProviderDescriptor` looks like (even just the Rust struct), I can sketch the exact additions needed to support typed predicate contracts + scope packs + derived predicate composition, while keeping your existing runtime resolution design intact.

Nice — that `ProviderDescriptor` is already *most of the machinery you need* for “scope-provider libraries.” You’ve basically built a mini ABI (application binary interface) for predicates.

Now the key is to tighten **what the provider contract means**, so the DSL can import it and compose it safely.

## 1) Your current shape is good, but `params.facts` is the wrong boundary

Right now each predicate schema requires `"facts": { "type": "array" }`.

That’s a smell because it pushes **provider-internal data plumbing** into the predicate API. In a clean model:

* provider snapshots facts (deterministically) into a facts bundle artifact
* predicate params should be *policy inputs* only (thresholds, patterns, options)
* the runtime automatically supplies the facts for the scope

So `lines_exceed` should look like:

```json
{
  "type": "object",
  "required": ["max_lines"],
  "properties": {
    "max_lines": { "type": "integer", "minimum": 0 }
  },
  "additionalProperties": false
}
```

Same for bytes / line length / todo. The predicate evaluates against the scope’s snapshot facts implicitly.

**Why this matters:** it makes DSL composition feel like “calling library predicates” instead of “manually threading arrays around.”

If you *do* need a selector, make it a real param like `include_globs` / `exclude_globs`, not raw facts.

## 2) Add stable IDs for predicates (don’t use only `name`)

Names are for humans. IDs are for contracts.

Extend `PredicateDescriptor` to include:

* `predicate_id: "text.metrics/lines_exceed@1"` (or derived from scope+name+version)
* `result_type: bool3 | findings | number | set[path]`
* `emits_findings: true/false`
* `evidence_schema` (optional, shape of finding payload)

This prevents collisions when you eventually have:

* `lines_exceed` in multiple scopes
* org overrides / forks

## 3) Treat `ProviderDescriptor` as a “scope pack” artifact

What you want for org extensibility is:

* a thing you can publish/version
* a thing the DSL can import
* a thing the registry can validate against

You already have it: `ProviderDescriptor` + predicate schemas.

Make it content-addressed:

* canonical CBOR encode the descriptor
* hash it (`provider_pack_hash`)
* put that hash in `meta-registry/1` under something like:

```json
{
  "scope_packs": [
    {
      "scope_id": "text.metrics",
      "version": 1,
      "provider_pack_hash": "sha256:...",
      "deterministic": true
    }
  ]
}
```

Now the DSL can import “scope packs” by `(scope_id@version)` and the runtime can verify it has an implementation that matches the pack hash.

That’s your “library system.”

## 4) Derived predicates: composition libraries without new code

Once the DSL can see the provider’s predicate signatures, you can allow:

```adm
import scope_pack "std/text.metrics@1"

derive predicate large_file() =
  text.metrics.bytes_exceed(max_bytes: 200000)
  or text.metrics.lines_exceed(max_lines: 2000)
```

This is *exactly* how orgs build reusable policy libraries without shipping new Rust.

## 5) Provider resolution: separate “semantic registry” from “runtime linking”

You want two layers:

* **Semantic layer (registry):** declares “scope X has predicates A,B,C with these params/types”
* **Runtime layer (provider registry):** provides an implementation for that scope pack

At runtime, you “link” by scope_id@version and verify the provider descriptor matches the registry’s expected pack hash.

This avoids the plugin swamp because you can reject mismatches deterministically.

## 6) Determinism metadata becomes enforceable (not just informational)

You already have `deterministic: true` + closure requirements.

Good. Next level is: if a provider is non-deterministic, it must return `bool3` (can be `unknown`) for anything that depends on the flaky world, and the lens must handle `unknown` explicitly.

So for GitHub-ish scopes:

* prefer `bool3` outputs
* prefer facts like `github_scope_available = false` rather than inventing results

That keeps CI honest.

## 7) About the witness comment screenshot

That PR comment is exactly the UX you want: the bot is basically saying:

> “Snapshot S, evaluated under config hash C, produced witness W, verdict admissible.”

To add multi-lens / bots / plans cleanly, you just extend that JSON summary with the same identity triangle:

* `snapshot_sha`
* `lens_hash` (+ activation event)
* `witness_hash`
* `plan_hash` (if present)
* `plan_contract` result
* `requires_manual_approval`

Then an agent can read the comment, parse the JSON, and know whether it’s allowed to iterate.

## 8) One concrete refactor suggestion for your TextMetricsProvider

Make the provider snapshot produce a stable fact set like:

* per file: `{path, lines, bytes, max_line_len, has_todo}`

Then predicates become “query patterns” over that set:

* `lines_exceed(max_lines, include_globs?, exclude_globs?) -> findings`
* `todo_present(include_globs?, markers?) -> findings`

No `facts` param. Cleaner contract. Much easier to compose.

---

### The punchline

You don’t need a giant new plugin architecture. You already have the right spine:

**ProviderDescriptor + typed PredicateDescriptor = scope library contract.**

To make it extensible and composable:

1. remove plumbing params like `facts`
2. add stable predicate IDs + result types
3. content-address the provider descriptor as a “scope pack”
4. let DSL import scope packs and define derived predicates

That gets you org-level libraries with deterministic governance and without “plugin chaos.”

If you show me your `PredicateDescriptor` struct definition and how `eval_predicate` currently receives params + facts, I can propose the minimal diff to remove `facts` from param_schema and move it into the implicit scope snapshot pipeline.
